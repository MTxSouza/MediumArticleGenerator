{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5b9modwDM5P"
      },
      "source": [
        "# Medium Article Generator\n",
        "**Author:** Matheus Oliveira de Souza - *Artificial Intelligence Developer*\n",
        "\n",
        "**References:**\n",
        "- [Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "**Dataset:**\n",
        "- [190k+ Medium Articles](https://www.kaggle.com/datasets/fabiochiusano/medium-articles)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJB0foIIJcO"
      },
      "source": [
        "# [1] Notebook setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4K8alEVIg9c"
      },
      "source": [
        "## [1.1] Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NOC9gysIVvc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# OpenAI Tokenizer\n",
        "import tiktoken\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gc\n",
        "from typing import Union, Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [1.2] Root path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT_PATH = os.path.abspath(path=os.path.dirname(p=\".\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYcOlXCnLyJP"
      },
      "source": [
        "# [2] Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXXGjnA7Lzpi"
      },
      "source": [
        "## [2.1] Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VzAGqRY0Udc"
      },
      "source": [
        "### [2.1.1] TikToken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aM_ZjdrL1bp"
      },
      "outputs": [],
      "source": [
        "class TikTokenizer:\n",
        "\n",
        "    cl100k_base = tiktoken.get_encoding(encoding_name=\"cl100k_base\")\n",
        "    SOS = \"<|sos|>\"\n",
        "    EOS = \"<|eos|>\"\n",
        "    UNK = \"<|unk|>\"\n",
        "    enc = tiktoken.Encoding(\n",
        "        name=\"cl100k_im\",\n",
        "        pat_str=cl100k_base._pat_str,\n",
        "        mergeable_ranks=cl100k_base._mergeable_ranks,\n",
        "        special_tokens={\n",
        "            **cl100k_base._special_tokens,\n",
        "            SOS: 100264,\n",
        "            EOS: 100265,\n",
        "            UNK: 100266,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    @classmethod\n",
        "    def vocab_size(cls) -> int:\n",
        "        \"\"\"Return the size of vocabulary.\"\"\"\n",
        "        return cls.enc.max_token_value\n",
        "\n",
        "    @classmethod\n",
        "    def encode(cls, text: str) -> list[int]:\n",
        "        \"\"\"Convert a sequence of characters into a sequence of numbers/indices.\"\"\"\n",
        "        return cls.enc.encode(text=text, allowed_special={cls.SOS, cls.EOS, cls.UNK})\n",
        "\n",
        "    @classmethod\n",
        "    def decode(cls, tokens: list[int]) -> str:\n",
        "        \"\"\"Convert a sequence of numbers/indices into a sequence of characters.\"\"\"\n",
        "        return cls.enc.decode(tokens=tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAcH9wZS0W9i"
      },
      "source": [
        "### [2.1.2] Custom Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9VsMo6N0ZXw"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self, vocab: dict[int, str], lookup_vocab: dict[int, int]) -> None:\n",
        "        self.vocab = {int(k): v for k, v in vocab.items()}\n",
        "        self.lookup = {int(k): int(v) for k, v in lookup_vocab.items()}\n",
        "        self.unk = self.lookup.get(TikTokenizer.encode(text=TikTokenizer.UNK)[0])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return the size of vocabulary.\"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Convert a sequence of characters into a sequence of numbers/indices.\"\"\"\n",
        "        tik_tokens = TikTokenizer.encode(text=text)\n",
        "        custom_tokens = [self.lookup.get(tk, self.unk) for tk in tik_tokens]\n",
        "        return custom_tokens\n",
        "\n",
        "    def decode(self, tokens: list[int], apply_join: bool = True) -> str:\n",
        "        \"\"\"Convert a sequence of numbers/indices into a sequence of characters.\"\"\"\n",
        "        string = [self.vocab.get(tk) for tk in tokens]\n",
        "        if apply_join:\n",
        "            return \"\".join(string)\n",
        "        return string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaEzPQezOp-W"
      },
      "source": [
        "## [2.2] Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-MNT9rTOsG6"
      },
      "outputs": [],
      "source": [
        "def save_losses(losses: list[float], dir_path: str, filename: str) -> None:\n",
        "    \"\"\"Save losses to a file.\"\"\"\n",
        "    with open(file=os.path.join(dir_path, filename), mode=\"w\") as json_buffer:\n",
        "        json.dump(obj={\"losses\": losses}, fp=json_buffer)\n",
        "\n",
        "def save_accuracies(accuracies: list[float], dir_path: str, filename: str) -> None:\n",
        "    \"\"\"Save accuracies to a file.\"\"\"\n",
        "    with open(file=os.path.join(dir_path, filename), mode=\"w\") as json_buffer:\n",
        "        json.dump(obj={\"accuracies\": accuracies}, fp=json_buffer)\n",
        "\n",
        "def save_train_stats(\n",
        "        best_loss: float,\n",
        "        curr_iter: int,\n",
        "        last_save: int,\n",
        "        overfitting: int,\n",
        "        last_train_loss: float,\n",
        "        last_valid_loss: float,\n",
        "        dir_path: str,\n",
        "        filename: str\n",
        "    ) -> None:\n",
        "    \"\"\"Save train stats.\"\"\"\n",
        "    with open(file=os.path.join(dir_path, filename), mode=\"w\") as json_buffer:\n",
        "        json.dump(obj={\"best_loss\": best_loss, \"curr_iter\": curr_iter, \"last_save\": last_save, \"overfitting\": overfitting, \"last_train_loss\": last_train_loss, \"last_valid_loss\": last_valid_loss}, fp=json_buffer)\n",
        "\n",
        "def save_vocab(vocab: dict, dir_path: str, filename: str) -> None:\n",
        "    \"\"\"Save vocab.\"\"\"\n",
        "    with open(file=os.path.join(dir_path, filename), mode=\"w\") as json_buffer:\n",
        "        json.dump(obj=vocab, fp=json_buffer)\n",
        "\n",
        "def load_json(file_path: str) -> dict:\n",
        "    \"\"\"Load json file.\"\"\"\n",
        "    with open(file=file_path, mode=\"r\") as json_buffer:\n",
        "        return json.load(fp=json_buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ9ybIJzJX0P"
      },
      "source": [
        "# [3] Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvYmOjBaJqwq"
      },
      "source": [
        "## [3.1] Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uo31tjRJoEy"
      },
      "outputs": [],
      "source": [
        "with open(file=os.path.join(ROOT_PATH, \"medium_articles.csv\"), mode=\"r\") as file_buffer:\n",
        "  reader = csv.reader(file_buffer)\n",
        "  next(reader)\n",
        "  dataset = list(reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZrGg61wJ4c7",
        "outputId": "c1dd23e9-9763-42be-dd14-b8aba1c66757"
      },
      "outputs": [],
      "source": [
        "print(f\"{len(dataset)=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06OmQdNg1sNs",
        "outputId": "43c26906-ce53-4b1a-8368-44a502ac28e9"
      },
      "outputs": [],
      "source": [
        "title_article_dataset = [[row[0], row[1], row[-1]] for row in dataset]\n",
        "del dataset\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_yxCyJr9Mtc"
      },
      "outputs": [],
      "source": [
        "full_dataset = \"\".join(f\"{TikTokenizer.SOS}{title}\\n\\n\\t{text}{TikTokenizer.EOS}\" for title, text, _ in title_article_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzAfJa0yKbns"
      },
      "source": [
        "## [3.2] Clear dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH6HlKzUKe8L"
      },
      "source": [
        "### [3.2.1] Unique characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkGjo9uLJ58T",
        "outputId": "b025d929-64e2-45e7-e2ff-f1f58be15c6b"
      },
      "outputs": [],
      "source": [
        "dataset_characters = \"\".join(sorted(list(set(full_dataset))))\n",
        "print(f\"{dataset_characters=}\")\n",
        "print(f\"{len(dataset_characters)=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OiOAQbO7Qqd"
      },
      "source": [
        "#### [3.2.1.1] Removing non UTF-8 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scyyQ7x77V6r"
      },
      "outputs": [],
      "source": [
        "non_utf8_characters_filter = re.compile(pattern=r\"[^\\x00-\\x7F]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl1OwMKz9KyJ",
        "outputId": "c61bbad2-7638-43ee-d611-fb92fbe00c39"
      },
      "outputs": [],
      "source": [
        "non_utf8_characters_list = non_utf8_characters_filter.findall(string=full_dataset)\n",
        "non_utf8_characters = \"\".join(list(set(non_utf8_characters_list)))\n",
        "print(f\"{len(non_utf8_characters_list)=}\")\n",
        "print(f\"{non_utf8_characters=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgQJOEG7-BWk",
        "outputId": "52113526-d10e-4ed5-98ea-3b2bf05387e8"
      },
      "outputs": [],
      "source": [
        "clean_full_dataset = non_utf8_characters_filter.sub(repl=\"\", string=full_dataset)\n",
        "del full_dataset, non_utf8_characters, non_utf8_characters_list\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l2YE25s-i0l",
        "outputId": "6f944793-181f-4dae-c082-b220cc4e94cc"
      },
      "outputs": [],
      "source": [
        "clean_dataset_characters = \"\".join(sorted(list(set(clean_full_dataset))))\n",
        "print(f\"{clean_dataset_characters=}\")\n",
        "print(f\"{len(clean_dataset_characters)=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMOogAkJ_Z5O"
      },
      "source": [
        "### [3.2.2] Checking topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8_Wriel_dI0"
      },
      "outputs": [],
      "source": [
        "title_filter = re.compile(pattern=rf\"{TikTokenizer.EOS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYactW8R_yJI"
      },
      "outputs": [],
      "source": [
        "title_list = title_filter.split(string=clean_full_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxB3ihHBAf61",
        "outputId": "4482c07c-eb2b-4c53-98a5-91b60dff22d1"
      },
      "outputs": [],
      "source": [
        "nltk.download(info_or_id=\"stopwords\")\n",
        "stop_words = list(set(stopwords.words(\"english\"))) + [\n",
        "    \":\",\n",
        "    \"?\",\n",
        "    \"(\",\n",
        "    \")\",\n",
        "    \"-\",\n",
        "    \".\",\n",
        "    \"&\",\n",
        "    \",\",\n",
        "    \"—\",\n",
        "    \"’\",\n",
        "    \"…\",\n",
        "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n",
        "    \"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAx8TViBoAA"
      },
      "source": [
        "#### [3.2.2.1] Removing stopword from titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rv9bOmaAKcS",
        "outputId": "9abf0505-7ec7-475a-863c-ec57a2390b84"
      },
      "outputs": [],
      "source": [
        "clean_title_list = []\n",
        "for content in tqdm(iterable=title_list):\n",
        "    title = content.replace(TikTokenizer.SOS, \"\").split(sep=\"\\n\\n\\t\")\n",
        "    if len(title) != 2:\n",
        "        continue\n",
        "    title = title[0]\n",
        "    tokens = TikTokenizer.encode(text=title)\n",
        "    str_tokens = [TikTokenizer.decode(tokens=[tk]).strip() for tk in tokens]\n",
        "    clean_title_list.append(\" \".join(stk for stk in str_tokens if stk not in stop_words and len(stk) > 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEsKnFBnBnHN"
      },
      "outputs": [],
      "source": [
        "count_title_words = Counter(\" \".join(clean_title_list).split(sep=\" \"))\n",
        "count_title_words.pop(\"\")\n",
        "top_count_title_words = count_title_words.most_common(n=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "mE9_bA0zCOvJ",
        "outputId": "87e6d075-f304-43ca-891c-9f9d2956f380"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "for word, count in top_count_title_words:\n",
        "    plt.bar(x=word, height=count)\n",
        "    plt.xticks(rotation=80)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ya9zYbCZMT"
      },
      "source": [
        "### [3.2.3] Removing links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85voFUk9CYdm"
      },
      "outputs": [],
      "source": [
        "link_pattern = re.compile(pattern=r\"https?://\\S+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADo3BzedCh75"
      },
      "outputs": [],
      "source": [
        "clean_full_dataset = link_pattern.sub(repl=\"\", string=clean_full_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KSduXbWC716",
        "outputId": "88a59599-3141-4ca4-a066-d891dfa8e22f"
      },
      "outputs": [],
      "source": [
        "print(f\"{len(clean_full_dataset)=} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67O2nGP3z5_p"
      },
      "source": [
        "### [3.2.4] Removing bad articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfrif50DWHSE",
        "outputId": "35bc84d5-415e-40fb-cba4-cf85298f5ddf"
      },
      "outputs": [],
      "source": [
        "splitted_dataset = title_filter.split(string=clean_full_dataset)\n",
        "print(f\"{len(splitted_dataset)=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8t9NiBNaFPb"
      },
      "source": [
        "#### [3.2.4.1] Removing empty articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzHdGM4zaJiV"
      },
      "outputs": [],
      "source": [
        "line_filter = re.compile(pattern=r\"(\\n){2,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urp_0RimZ9Ur",
        "outputId": "e2a8686e-e8db-4d3a-aebb-cc936bebb45c"
      },
      "outputs": [],
      "source": [
        "valid_text = []\n",
        "for content in tqdm(iterable=splitted_dataset):\n",
        "    f = content.replace(TikTokenizer.SOS, \"\").split(sep=\"\\n\\n\\t\")\n",
        "    if len(f) != 2:\n",
        "        continue\n",
        "    title, article = f\n",
        "    article = line_filter.sub(repl=\"\\n\", string=article)\n",
        "    article = re.sub(pattern=r\" {1,}\\n\", repl=\"\\n\", string=article)\n",
        "    article = article.strip() + \"\\n\"\n",
        "    if len(article) > 1000: # character limit of article\n",
        "        title = title.strip()\n",
        "        valid_text.append([title, article])\n",
        "print(f\"{len(valid_text)=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQWejg2PynK3"
      },
      "source": [
        "### [3.2.5] Checking article sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-__rN2UysM_",
        "outputId": "14934358-f2d7-4c42-ad71-2a39d1154ec2"
      },
      "outputs": [],
      "source": [
        "print(f\"Longest : {max(len(article) for title, article in valid_text)}\")\n",
        "print(f\"Shortest : {min(len(article) for title, article in valid_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTmuaxZyy9wr"
      },
      "source": [
        "### [3.2.6] Final dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n9V4EC9d588",
        "outputId": "09cb83d6-dce3-49dc-9757-a3b7413f0591"
      },
      "outputs": [],
      "source": [
        "clean_full_dataset = \"\".join(list(map(lambda x: TikTokenizer.SOS + \"\\t\\n\\n\".join(x) + TikTokenizer.EOS, valid_text))).strip()\n",
        "print(f\"{len(clean_full_dataset)=} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LKPYkn22rg0"
      },
      "source": [
        "## [3.3] Free memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is74Ck0T2vsW",
        "outputId": "a3aa9806-2d97-44b2-ce4c-fe2bf02350ba"
      },
      "outputs": [],
      "source": [
        "del valid_text, count_title_words, top_count_title_words, clean_title_list, title_list\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwOyhgKHPfTh"
      },
      "source": [
        "# [4] Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9UaelFUPp8X"
      },
      "source": [
        "## [4.1] Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E5L7X2iQObh"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, context: int, emb_dim: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        even_i = torch.arange(start=0, end=emb_dim, step=2).float()\n",
        "        odd_i = torch.arange(start=1, end=emb_dim, step=2).float() - 1\n",
        "\n",
        "        even_denom = torch.pow(10_000, exponent=even_i / emb_dim)\n",
        "        odd_denom = torch.pow(10_000, exponent=odd_i / emb_dim)\n",
        "\n",
        "        pos = torch.arange(end=context).float().reshape(shape=[context, 1])\n",
        "\n",
        "        even = torch.sin(pos / even_denom)\n",
        "        odd = torch.cos(pos / odd_denom)\n",
        "\n",
        "        self.register_buffer(name=\"pe\", tensor=torch.cat(tensors=[even, odd], dim=1).expand(1, -1, -1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, D = x.size()\n",
        "        x_pe = x + self.pe[:,:T,:]\n",
        "        return x_pe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3meI_d7LUYPm"
      },
      "source": [
        "## [4.2] Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDfM5KySUICG"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim: int, ff_dim: int, dropout_rate: float = 0.2) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(in_features=emb_dim, out_features=ff_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.linear_2 = nn.Linear(in_features=ff_dim, out_features=emb_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z1 = self.linear_1(x)\n",
        "        a1 = self.relu(z1)\n",
        "        a1 = self.dropout(a1)\n",
        "        z2 = self.linear_2(a1)\n",
        "        return z2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCLfGJ0SU0PJ"
      },
      "source": [
        "## [4.3] Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz5Qy2ElUzZz"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim: int, head_dim: int, context: int, dropout_rate: float) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.query = nn.Linear(in_features=emb_dim, out_features=head_dim, bias=False)\n",
        "        self.key = nn.Linear(in_features=emb_dim, out_features=head_dim, bias=False)\n",
        "        self.value = nn.Linear(in_features=emb_dim, out_features=head_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        ones = torch.ones(size=[context, context], dtype=torch.float)\n",
        "        self.register_buffer(name=\"mask\", tensor=torch.tril(input=ones))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        B, T, C = x.size()\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        QK = Q @ K.transpose(-2, -1) * C**-0.5\n",
        "        attention = QK.masked_fill(self.mask[:T,:T] == 0, float(\"-inf\"))\n",
        "        attention = F.softmax(input=attention, dim=-1)\n",
        "\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        out = attention @ V\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim: int, head_dim: int, context: int, dropout_rate: float) -> None:\n",
        "        super().__init__()\n",
        "        n_heads = emb_dim // head_dim\n",
        "        self.attention = nn.ModuleList(modules=[Attention(emb_dim=emb_dim, head_dim=head_dim, context=context, dropout_rate=dropout_rate) for _ in range(n_heads)])\n",
        "        self.linear = nn.Linear(in_features=emb_dim, out_features=emb_dim)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = torch.cat(tensors=[attention(x) for attention in self.attention], dim=-1)\n",
        "        out = self.linear(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSKUetKQWZ1d"
      },
      "source": [
        "## [4.4] Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvV68WaQWQ7s"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim: int, head_dim: int, context: int, ff_dim: int, dropout_rate: float) -> None:\n",
        "        super().__init__()\n",
        "        self.attention = MultiAttention(emb_dim=emb_dim, head_dim=head_dim, context=context, dropout_rate=dropout_rate)\n",
        "        self.feed_forward = FeedForward(emb_dim=emb_dim, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
        "        self.norm_1 = nn.LayerNorm(normalized_shape=emb_dim)\n",
        "        self.norm_2 = nn.LayerNorm(normalized_shape=emb_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_norm = self.norm_1(x)\n",
        "        attention = self.attention(x_norm)\n",
        "        attention = attention + x\n",
        "\n",
        "        attention_norm = self.norm_2(attention)\n",
        "        ff = self.feed_forward(attention_norm)\n",
        "        ff = ff + attention\n",
        "\n",
        "        return ff\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers: int, decoder: DecoderLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(*[decoder for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUAwhxtwZGwH"
      },
      "source": [
        "## [4.5] Article Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYtYeLZAXy3L"
      },
      "outputs": [],
      "source": [
        "class ArticleGenerator(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_layers: int,\n",
        "            vocab_size: int,\n",
        "            emb_dim: int,\n",
        "            head_dim: int,\n",
        "            context: int,\n",
        "            ff_dim: int,\n",
        "            dropout_rate: float,\n",
        "            device: str,\n",
        "            tokenizer: Tokenizer\n",
        "        ) -> None:\n",
        "        super().__init__()\n",
        "        self.ctx = context\n",
        "        self.eos = vocab_size - 1\n",
        "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
        "        self.pe = PositionalEncoding(context=context, emb_dim=emb_dim)\n",
        "        self.layers = nn.Sequential(*[DecoderLayer(emb_dim=emb_dim, head_dim=head_dim, context=context, ff_dim=ff_dim, dropout_rate=dropout_rate) for _ in range(n_layers)])\n",
        "        self.norm = nn.LayerNorm(normalized_shape=emb_dim)\n",
        "        self.out = nn.Linear(in_features=emb_dim, out_features=vocab_size)\n",
        "\n",
        "        self.dev = device\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.emb(x)\n",
        "        x = self.pe(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_next_token(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        x = self(x)\n",
        "        token = x.argmax(dim=-1)[:,-1]\n",
        "        return token\n",
        "\n",
        "    def generate(self, text: str = \"<|sos|>\", max_len: int = None) -> Generator[str, None, None]:\n",
        "        max_len = float(\"inf\") if max_len is None else max_len\n",
        "        count = 0\n",
        "        x = torch.tensor(data=self.tokenizer.encode(text=text), requires_grad=False).unsqueeze(dim=0).to(device=self.dev)\n",
        "        assert x.ndim == 2\n",
        "        while count < max_len:\n",
        "            if x.size(dim=1) > self.ctx:\n",
        "                x = x[:,1:] # ignoring first token of window context\n",
        "            token = self.predict_next_token(x)\n",
        "            yield self.tokenizer.decode(tokens=[token.item()])\n",
        "            x = torch.cat(tensors=[x, token.unsqueeze(dim=0)], dim=1)\n",
        "            count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQxEmAQ2gsqQ"
      },
      "source": [
        "# [5] Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKYLZh9WgvRo"
      },
      "source": [
        "## [5.1] Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpugHZgvfqv1"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "N_LAYERS = 6\n",
        "EMB_DIM = 768\n",
        "HEAD_DIM = 64\n",
        "FF_DIM = EMB_DIM * 4\n",
        "DROPOUT = 0.05\n",
        "WINDOW_CONTEXT = 512\n",
        "\n",
        "# train\n",
        "SEED = 1234\n",
        "EPOCHS = 1000\n",
        "EARLY_STOP = 10\n",
        "TRAIN_ITERATIONS = 400\n",
        "VALID_ITERATIONS = 200\n",
        "TRAIN_SPLIT = 0.8\n",
        "LR = 2e-4\n",
        "WEIGHT_DECAY = 0\n",
        "BATCH_SIZE = 24\n",
        "\n",
        "WEIGHT_FOLDER = os.path.join(ROOT_PATH, \"exp\")\n",
        "if not os.path.exists(path=WEIGHT_FOLDER):\n",
        "    os.mkdir(path=WEIGHT_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBm02EH_hCJn"
      },
      "source": [
        "## [5.2] Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_RgAzbLhBj1"
      },
      "outputs": [],
      "source": [
        "class ArticleDataset(Dataset):\n",
        "\n",
        "    def __init__(self, articles: list[int], context: int, n_iter: int, batch_size: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.len = int(n_iter * batch_size)\n",
        "        self.x = articles\n",
        "        self.ctx = context\n",
        "        self.limit = len(articles) - self.ctx - 1\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index: int) -> Union[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        index_content = np.random.randint(low=0, high=self.limit, size=1).item()\n",
        "\n",
        "        x = self.x[index_content:index_content+self.ctx]\n",
        "        y = self.x[index_content+1:index_content+self.ctx+1]\n",
        "\n",
        "        np_x = np.asarray(a=x, dtype=np.int64)\n",
        "        np_y = np.asarray(a=y, dtype=np.int64)\n",
        "\n",
        "        t_x = torch.tensor(data=np_x, requires_grad=False).long()\n",
        "        t_y = torch.tensor(data=np_y, requires_grad=False).long()\n",
        "\n",
        "        return t_x, t_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkdfxKGt2E3e"
      },
      "source": [
        "### [5.2.1] Defining tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce8C126CAOLu",
        "outputId": "bbd50c33-7bd5-433f-8d15-65cd88bf11b0"
      },
      "outputs": [],
      "source": [
        "unique_tokens = sorted(list(set(TikTokenizer.encode(text=clean_full_dataset + TikTokenizer.UNK))))\n",
        "print(f\"{len(unique_tokens)=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXlSFzJ02Qph",
        "outputId": "110dee02-46de-485d-cffd-493d5830453f"
      },
      "outputs": [],
      "source": [
        "custom_vocab = {}\n",
        "vocab_mapper = {}\n",
        "for unique_tk in tqdm(iterable=unique_tokens):\n",
        "    str_token = TikTokenizer.decode(tokens=[unique_tk])\n",
        "    idx = len(custom_vocab)\n",
        "    custom_vocab[idx] = str_token\n",
        "    vocab_mapper[unique_tk] = idx\n",
        "\n",
        "del unique_tokens\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d0lh1ya7SfI"
      },
      "outputs": [],
      "source": [
        "save_vocab(vocab=custom_vocab, dir_path=WEIGHT_FOLDER, filename=\"vocab.json\")\n",
        "save_vocab(vocab=vocab_mapper, dir_path=WEIGHT_FOLDER, filename=\"mapper.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEBPLQwR2LJN"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(vocab=custom_vocab, lookup_vocab=vocab_mapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiV3J4ulFeqz"
      },
      "outputs": [],
      "source": [
        "clean_full_tk_dataset = tokenizer.encode(text=clean_full_dataset)\n",
        "del clean_full_dataset\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9JvS4Wdwsqr"
      },
      "source": [
        "### [5.2.2] Splitting dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjeTjGsiwu3S",
        "outputId": "8713e352-e981-4aa2-bb2c-434cc41d70df"
      },
      "outputs": [],
      "source": [
        "train_split = int(len(clean_full_tk_dataset) * TRAIN_SPLIT)\n",
        "print(f\"{train_split=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSc1kLTXxCwE",
        "outputId": "3cefe07e-cbc2-4d3c-cd64-baabc79c015f"
      },
      "outputs": [],
      "source": [
        "train_set = clean_full_tk_dataset[:train_split]\n",
        "valid_set = clean_full_tk_dataset[train_split:]\n",
        "del clean_full_tk_dataset\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4o7cq9AoqSU"
      },
      "source": [
        "### [5.2.3] Creating dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYLqlnbziopG",
        "outputId": "6f1d7bbc-3dd2-40c4-96c9-d23a08ba44ec"
      },
      "outputs": [],
      "source": [
        "print(f\"{len(tokenizer)=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r51gCoujEZ_",
        "outputId": "ebff1b57-605a-4ed8-da43-f96e71415b59"
      },
      "outputs": [],
      "source": [
        "print(f\"{len(train_set)=}\")\n",
        "print(f\"{len(valid_set)=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4VLUKxQxZm3"
      },
      "outputs": [],
      "source": [
        "train_dataset = ArticleDataset(articles=train_set, context=WINDOW_CONTEXT, n_iter=TRAIN_ITERATIONS, batch_size=BATCH_SIZE)\n",
        "valid_dataset = ArticleDataset(articles=valid_set, context=WINDOW_CONTEXT, n_iter=VALID_ITERATIONS, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxbVs4bW5JyI",
        "outputId": "c6580c10-1ca1-41e9-b674-ecab3d16252a"
      },
      "outputs": [],
      "source": [
        "for _ in range(2):\n",
        "    for x, y in train_dataset:\n",
        "        print(f\"{x.shape=}\")\n",
        "        print(f\"{y.shape=}\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"x: {tokenizer.decode(tokens=x.tolist())}\")\n",
        "        print(f\"\\ny: {tokenizer.decode(tokens=y.tolist())}\")\n",
        "\n",
        "        print(\"=\" * 100)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgtRmmMZPhxu"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSuRAMNrxhcQ"
      },
      "source": [
        "## [5.3] Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBpRhVAevY2Z",
        "outputId": "5f9f3588-e3b1-4f82-a804-b52e53c80d74"
      },
      "outputs": [],
      "source": [
        "device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"{device=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5PamvwivMp8"
      },
      "source": [
        "## [5.4] Instanciating model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qth9CwFHtm3J",
        "outputId": "30d64042-1ac0-4371-fb19-82ea4eb7480b"
      },
      "outputs": [],
      "source": [
        "model = ArticleGenerator(\n",
        "    n_layers=N_LAYERS,\n",
        "    vocab_size=len(tokenizer),\n",
        "    emb_dim=EMB_DIM,\n",
        "    head_dim=HEAD_DIM,\n",
        "    context=WINDOW_CONTEXT,\n",
        "    ff_dim=FF_DIM,\n",
        "    dropout_rate=DROPOUT,\n",
        "    device=device,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "model = nn.DataParallel(module=model) # multiple GPUs\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8mMB6Hp2Zro",
        "outputId": "c54efc79-3e96-4286-e976-fae1e9bb8aaf"
      },
      "outputs": [],
      "source": [
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "buffer_size = sum(p.nelement() * p.element_size() for p in model.buffers())\n",
        "model_size = (n_parameters + buffer_size) / 1024**2\n",
        "print(f\"{n_parameters=}\")\n",
        "print(f\"{model_size=:.2f}Mb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeB98DlJ1Vyi",
        "outputId": "b9c34813-3df0-4c93-d7bc-58a27ccaf320"
      },
      "outputs": [],
      "source": [
        "def init_weights(module: nn.Module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_normal_(tensor=module.weight)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(tensor=module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.xavier_normal_(tensor=module.weight)\n",
        "\n",
        "model.apply(init_weights)\n",
        "model.to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rArw33X4NVYI"
      },
      "source": [
        "### [5.4.1] Model loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoLjZHw3NaOJ"
      },
      "outputs": [],
      "source": [
        "def model_metric(yhat: torch.Tensor, y: torch.Tensor, tokenizer: Tokenizer):\n",
        "\n",
        "    batch_size, ctx, _ = yhat.size()\n",
        "\n",
        "    base_yhat = yhat.view(batch_size * ctx, -1)\n",
        "    base_y = y.view(-1)\n",
        "\n",
        "    loss = F.cross_entropy(input=base_yhat, target=base_y)\n",
        "\n",
        "    pred = yhat.argmax(dim=-1)\n",
        "    pred_tokens = [tokenizer.decode(tokens=tokens, apply_join=False) for tokens in pred.tolist()]\n",
        "    true_tokens = [tokenizer.decode(tokens=tokens, apply_join=False) for tokens in y.tolist()]\n",
        "\n",
        "    accuracies = [sentence_bleu(references=[true], hypothesis=pred, smoothing_function=SmoothingFunction().method1) for pred, true in zip(pred_tokens, true_tokens)]\n",
        "    acc = sum(accuracies) / batch_size\n",
        "\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N-01Q6IxU4p"
      },
      "source": [
        "### [5.4.2] Checking loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbvSthx2yjok",
        "outputId": "37e27397-cf47-4a08-dd06-58c54146e157"
      },
      "outputs": [],
      "source": [
        "expected_loss = -np.log(1 / len(tokenizer)) # all classes has the same probability to be predicted.\n",
        "print(f\"{expected_loss=}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in valid_loader:\n",
        "        x, y = x.to(device=device), y.to(device=device)\n",
        "        print(f\"{x.shape=}\")\n",
        "        print(f\"{y.shape=}\")\n",
        "        yhat = model(x)\n",
        "        loss, acc = model_metric(yhat=yhat, y=y, tokenizer=tokenizer)\n",
        "        print(f\"{loss=}\")\n",
        "        print(f\"{acc=}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkCW1EgAx_AC"
      },
      "source": [
        "## [5.5] Train config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_QxKctC73Wz"
      },
      "outputs": [],
      "source": [
        "if False: # continue training\n",
        "    stats = load_json(file_path=os.path.join(WEIGHT_FOLDER, \"stats.json\"))\n",
        "    curr_iter = stats.get(\"curr_iter\")\n",
        "    last_save = stats.get(\"last_save\")\n",
        "    best_loss = stats.get(\"best_loss\")\n",
        "    last_train_loss = stats.get(\"last_train_loss\")\n",
        "    last_valid_loss = stats.get(\"last_valid_loss\")\n",
        "    overfitting = stats.get(\"overfitting\")\n",
        "    train_losses = load_json(file_path=os.path.join(WEIGHT_FOLDER, \"train_losses.json\")).get(\"losses\")\n",
        "    valid_losses = load_json(file_path=os.path.join(WEIGHT_FOLDER, \"valid_losses.json\")).get(\"losses\")\n",
        "    train_accuracies = load_json(file_path=os.path.join(WEIGHT_FOLDER, \"train_accuracies.json\")).get(\"accuracies\")\n",
        "    valid_accuracies = load_json(file_path=os.path.join(WEIGHT_FOLDER, \"valid_accuracies.json\")).get(\"accuracies\")\n",
        "    model.load_state_dict(torch.load(f=os.path.join(WEIGHT_FOLDER, \"last_weights.pt\"), map_location=device))\n",
        "else:\n",
        "    curr_iter = 1\n",
        "    last_save = 0\n",
        "    best_loss = np.inf\n",
        "\n",
        "    last_train_loss = 0\n",
        "    last_valid_loss = 0\n",
        "    overfitting = 0\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_accuracies = []\n",
        "    valid_accuracies = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZWdZl5ux3cX"
      },
      "source": [
        "## [5.6] Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw8C-BxfxrXQ"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(params=model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIGYHiIgrNcO"
      },
      "source": [
        "## [5.7] Train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "DF7nLaSFyKXl",
        "outputId": "7ead8961-cb38-47ac-de8d-0ee5a84e4652"
      },
      "outputs": [],
      "source": [
        "while curr_iter < EPOCHS + 1:\n",
        "\n",
        "    print(f\"EPOCH {curr_iter}/{EPOCHS} | Overfitting {overfitting}/{EARLY_STOP} | Best valid loss {best_loss} | Last savement : {last_save}\")\n",
        "    save_train_stats(\n",
        "        best_loss=best_loss,\n",
        "        curr_iter=curr_iter,\n",
        "        last_save=last_save,\n",
        "        overfitting=overfitting,\n",
        "        last_train_loss=last_train_loss,\n",
        "        last_valid_loss=last_valid_loss,\n",
        "        dir_path=WEIGHT_FOLDER,\n",
        "        filename=\"stats.json\"\n",
        "    )\n",
        "\n",
        "    if overfitting == EARLY_STOP:\n",
        "        break\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_tqdm = tqdm(iterable=train_loader)\n",
        "    for x, y in train_tqdm:\n",
        "\n",
        "        x, y = x.to(device=device), y.to(device=device)\n",
        "        yhat = model(x)\n",
        "\n",
        "        loss, acc = model_metric(yhat=yhat, y=y, tokenizer=tokenizer)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        curr_loss = loss.cpu().item()\n",
        "        curr_acc = acc.cpu().item()\n",
        "        train_losses.append(curr_loss)\n",
        "        train_accuracies.append(curr_acc)\n",
        "        train_loss += curr_loss\n",
        "        train_acc += curr_acc\n",
        "\n",
        "        train_tqdm.set_description(desc=f\"loss : {curr_loss} - accuracy : {acc}\")\n",
        "    train_loss /= TRAIN_ITERATIONS\n",
        "    train_acc /= TRAIN_ITERATIONS\n",
        "    print(f\"[train] loss : {train_loss} - accuracy : {train_acc}\")\n",
        "\n",
        "    model.eval()\n",
        "    valid_tqdm = tqdm(iterable=valid_loader)\n",
        "    valid_loss = 0\n",
        "    valid_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in valid_tqdm:\n",
        "\n",
        "            x, y = x.to(device=device), y.to(device=device)\n",
        "            yhat = model(x)\n",
        "\n",
        "            loss, acc = model_metric(yhat=yhat, y=y, tokenizer=tokenizer)\n",
        "\n",
        "            curr_loss = loss.cpu().item()\n",
        "            curr_acc = acc.cpu().item()\n",
        "            valid_losses.append(curr_loss)\n",
        "            valid_accuracies.append(curr_acc)\n",
        "            valid_loss += curr_loss\n",
        "            valid_acc += curr_acc\n",
        "\n",
        "            valid_tqdm.set_description(desc=f\"loss : {curr_loss} - accuracy : {acc}\")\n",
        "        valid_loss /= VALID_ITERATIONS\n",
        "        valid_acc /= VALID_ITERATIONS\n",
        "        print(f\"[valid] loss : {valid_loss} - accuracy : {valid_acc}\")\n",
        "\n",
        "    if best_loss > valid_loss:\n",
        "        best_loss = valid_loss\n",
        "        last_save = curr_iter\n",
        "        torch.save(obj=model.state_dict(), f=os.path.join(WEIGHT_FOLDER, \"weights.pt\"))\n",
        "        overfitting = 0\n",
        "    elif last_train_loss > train_loss and valid_loss > last_valid_loss:\n",
        "        overfitting += 1\n",
        "    elif overfitting:\n",
        "        overfitting -= 1\n",
        "\n",
        "    curr_iter += 1\n",
        "    last_train_loss = train_loss\n",
        "    last_valid_loss = valid_loss\n",
        "\n",
        "    save_losses(losses=train_losses, dir_path=WEIGHT_FOLDER, filename=\"train_losses.json\")\n",
        "    save_losses(losses=valid_losses, dir_path=WEIGHT_FOLDER, filename=\"valid_losses.json\")\n",
        "    save_accuracies(accuracies=train_accuracies, dir_path=WEIGHT_FOLDER, filename=\"train_accuracies.json\")\n",
        "    save_accuracies(accuracies=valid_accuracies, dir_path=WEIGHT_FOLDER, filename=\"valid_accuracies.json\")\n",
        "    torch.save(obj=model.state_dict(), f=os.path.join(WEIGHT_FOLDER, \"last_weights.pt\"))\n",
        "    print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu1LOnqvPwQ2"
      },
      "source": [
        "# [6] Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBPnLQYYP1dl"
      },
      "source": [
        "## [6.1] Loading model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTB8uDwkmIEF"
      },
      "outputs": [],
      "source": [
        "custom_vocab = load_json(file_path=os.path.join(WEIGHT_FOLDER, \"vocab.json\"))\n",
        "vocab_mapper = load_json(file_path=os.path.join(WEIGHT_FOLDER, \"mapper.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4POLCBFnl5wB"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(vocab=custom_vocab, lookup_vocab=vocab_mapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZT9hZKxnksr",
        "outputId": "aad92085-c725-4a37-d571-e41ee4dfd184"
      },
      "outputs": [],
      "source": [
        "device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"{device=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyDXnLByl2FI",
        "outputId": "abea31f8-7e3d-469e-bb29-8949a760cb68"
      },
      "outputs": [],
      "source": [
        "model = ArticleGenerator(\n",
        "    n_layers=N_LAYERS,\n",
        "    vocab_size=len(tokenizer),\n",
        "    emb_dim=EMB_DIM,\n",
        "    head_dim=HEAD_DIM,\n",
        "    context=WINDOW_CONTEXT,\n",
        "    ff_dim=FF_DIM,\n",
        "    dropout_rate=DROPOUT,\n",
        "    device=device,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "model = nn.DataParallel(module=model)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KERNx9FEjIf",
        "outputId": "3547e7a0-ad0e-4713-cf04-1756c5a44e96"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(f=os.path.join(WEIGHT_FOLDER, \"weights.pt\"), map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXTmkiwuP8ZL"
      },
      "source": [
        "## [6.2] Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VI3o3mQP6pp",
        "outputId": "eba8686b-ec23-49e2-8cdb-13585463116c"
      },
      "outputs": [],
      "source": [
        "for i, token in enumerate(iterable=model.module.generate(max_len=100)):\n",
        "    print(token, end=\"\")\n",
        "    if i % 30 == 0: # break line\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lV8ebS_2ddA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w8gfZm4VIMCh",
        "ezZQFZR_Ika5",
        "H4K8alEVIg9c",
        "GYcOlXCnLyJP",
        "aQ9ybIJzJX0P",
        "UWTyNmyIJasR",
        "MwOyhgKHPfTh",
        "YBm02EH_hCJn",
        "LSuRAMNrxhcQ",
        "y5PamvwivMp8",
        "BkCW1EgAx_AC",
        "FZWdZl5ux3cX"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
